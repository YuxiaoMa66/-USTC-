{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\tf_torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  8 16:46:08 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 497.47       Driver Version: 497.47       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   41C    P0    31W /  N/A |    149MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read train files: 25000\n",
      "read test files: 25000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os \n",
    "def rm_tags(text):\n",
    "    re_tags = re.compile(r'<[^>]+>')\n",
    "    return re_tags.sub(' ',text)\n",
    "\n",
    "def read_files(filetype):\n",
    "    path = \"C:/Users/20123/Desktop/计算机/深度学习/lab3/.data/imdb/aclImdb/\"\n",
    "    file_list=[]\n",
    "    \n",
    "    positive_path = path + filetype + \"/pos/\"\n",
    "    for f in os.listdir(positive_path):\n",
    "        file_list += [positive_path + f]\n",
    "        \n",
    "    negative_path = path + filetype + \"/neg/\"\n",
    "    for f in os.listdir(negative_path):\n",
    "        file_list += [negative_path + f]   \n",
    "        \n",
    "    print(\"read\",filetype,\"files:\",len(file_list))\n",
    "    \n",
    "    all_labels = ([1]*12500+[0]*12500)\n",
    "    \n",
    "    all_texts = []\n",
    "    for fi in file_list:\n",
    "        with open(fi,encoding = 'utf8') as file_input:\n",
    "            all_texts += [rm_tags(\" \".join(file_input.readlines()))]\n",
    "            \n",
    "    return all_labels,all_texts\n",
    "\n",
    "y_train,train_text = read_files(\"train\")\n",
    "y_test,test_text = read_files(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "sentences=train_text\n",
    "labels=y_train\n",
    "\n",
    "test_sentences=test_text\n",
    "test_labels=y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "Tokenized:  ['bro', '##m', '##well', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'teachers', '\"', '.', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bro', '##m', '##well', 'high', \"'\", 's', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'teachers', '\"', '.', 'the', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insight', '##ful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'po', '##mp', ',', 'the', 'pet', '##tine', '##ss', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', '.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'i', 'immediately', 'recalled', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'at', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'high', '.', 'a', 'classic', 'line', ':', 'inspector', ':', 'i', \"'\", 'm', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'student', ':', 'welcome', 'to', 'bro', '##m', '##well', 'high', '.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bro', '##m', '##well', 'high', 'is', 'far', 'fetch', '##ed', '.', 'what', 'a', 'pity', 'that', 'it', 'isn', \"'\", 't', '!']\n",
      "Token IDs:  [22953, 2213, 4381, 2152, 2003, 1037, 9476, 4038, 1012, 2009, 2743, 2012, 1996, 2168, 2051, 2004, 2070, 2060, 3454, 2055, 2082, 2166, 1010, 2107, 2004, 1000, 5089, 1000, 1012, 2026, 3486, 2086, 1999, 1996, 4252, 9518, 2599, 2033, 2000, 2903, 2008, 22953, 2213, 4381, 2152, 1005, 1055, 18312, 2003, 2172, 3553, 2000, 4507, 2084, 2003, 1000, 5089, 1000, 1012, 1996, 25740, 2000, 5788, 13732, 1010, 1996, 12369, 3993, 2493, 2040, 2064, 2156, 2157, 2083, 2037, 17203, 5089, 1005, 13433, 8737, 1010, 1996, 9004, 10196, 4757, 1997, 1996, 2878, 3663, 1010, 2035, 10825, 2033, 1997, 1996, 2816, 1045, 2354, 1998, 2037, 2493, 1012, 2043, 1045, 2387, 1996, 2792, 1999, 2029, 1037, 3076, 8385, 2699, 2000, 6402, 2091, 1996, 2082, 1010, 1045, 3202, 7383, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 2012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 2152, 1012, 1037, 4438, 2240, 1024, 7742, 1024, 1045, 1005, 1049, 2182, 2000, 12803, 2028, 1997, 2115, 5089, 1012, 3076, 1024, 6160, 2000, 22953, 2213, 4381, 2152, 1012, 1045, 5987, 2008, 2116, 6001, 1997, 2026, 2287, 2228, 2008, 22953, 2213, 4381, 2152, 2003, 2521, 18584, 2098, 1012, 2054, 1037, 12063, 2008, 2009, 3475, 1005, 1056, 999]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN=128\n",
    "\n",
    "input_ids = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in sentences]\n",
    "test_input_ids=[tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in test_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)\n",
    "\n",
    "test_attention_masks = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in test_input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    test_attention_masks.append(att_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "# Do the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
    "                                             random_state=2020, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "test_inputs=torch.tensor(test_input_ids)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "test_labels=torch.tensor(test_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "test_masks=torch.tensor(test_attention_masks)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our test set.\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\tf_torch\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,407.    Elapsed: 0:00:09.\n",
      "  Batch    80  of  1,407.    Elapsed: 0:00:15.\n",
      "  Batch   120  of  1,407.    Elapsed: 0:00:21.\n",
      "  Batch   160  of  1,407.    Elapsed: 0:00:27.\n",
      "  Batch   200  of  1,407.    Elapsed: 0:00:33.\n",
      "  Batch   240  of  1,407.    Elapsed: 0:00:39.\n",
      "  Batch   280  of  1,407.    Elapsed: 0:00:45.\n",
      "  Batch   320  of  1,407.    Elapsed: 0:00:50.\n",
      "  Batch   360  of  1,407.    Elapsed: 0:00:56.\n",
      "  Batch   400  of  1,407.    Elapsed: 0:01:02.\n",
      "  Batch   440  of  1,407.    Elapsed: 0:01:09.\n",
      "  Batch   480  of  1,407.    Elapsed: 0:01:15.\n",
      "  Batch   520  of  1,407.    Elapsed: 0:01:21.\n",
      "  Batch   560  of  1,407.    Elapsed: 0:01:27.\n",
      "  Batch   600  of  1,407.    Elapsed: 0:01:33.\n",
      "  Batch   640  of  1,407.    Elapsed: 0:01:39.\n",
      "  Batch   680  of  1,407.    Elapsed: 0:01:45.\n",
      "  Batch   720  of  1,407.    Elapsed: 0:01:51.\n",
      "  Batch   760  of  1,407.    Elapsed: 0:01:56.\n",
      "  Batch   800  of  1,407.    Elapsed: 0:02:02.\n",
      "  Batch   840  of  1,407.    Elapsed: 0:02:08.\n",
      "  Batch   880  of  1,407.    Elapsed: 0:02:14.\n",
      "  Batch   920  of  1,407.    Elapsed: 0:02:20.\n",
      "  Batch   960  of  1,407.    Elapsed: 0:02:26.\n",
      "  Batch 1,000  of  1,407.    Elapsed: 0:02:32.\n",
      "  Batch 1,040  of  1,407.    Elapsed: 0:02:37.\n",
      "  Batch 1,080  of  1,407.    Elapsed: 0:02:43.\n",
      "  Batch 1,120  of  1,407.    Elapsed: 0:02:49.\n",
      "  Batch 1,160  of  1,407.    Elapsed: 0:02:55.\n",
      "  Batch 1,200  of  1,407.    Elapsed: 0:03:01.\n",
      "  Batch 1,240  of  1,407.    Elapsed: 0:03:07.\n",
      "  Batch 1,280  of  1,407.    Elapsed: 0:03:13.\n",
      "  Batch 1,320  of  1,407.    Elapsed: 0:03:19.\n",
      "  Batch 1,360  of  1,407.    Elapsed: 0:03:25.\n",
      "  Batch 1,400  of  1,407.    Elapsed: 0:03:31.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epcoh took: 0:03:31\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.88\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,407.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  1,407.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  1,407.    Elapsed: 0:00:18.\n",
      "  Batch   160  of  1,407.    Elapsed: 0:00:24.\n",
      "  Batch   200  of  1,407.    Elapsed: 0:00:30.\n",
      "  Batch   240  of  1,407.    Elapsed: 0:00:36.\n",
      "  Batch   280  of  1,407.    Elapsed: 0:00:41.\n",
      "  Batch   320  of  1,407.    Elapsed: 0:00:47.\n",
      "  Batch   360  of  1,407.    Elapsed: 0:00:53.\n",
      "  Batch   400  of  1,407.    Elapsed: 0:01:00.\n",
      "  Batch   440  of  1,407.    Elapsed: 0:01:06.\n",
      "  Batch   480  of  1,407.    Elapsed: 0:01:12.\n",
      "  Batch   520  of  1,407.    Elapsed: 0:01:17.\n",
      "  Batch   560  of  1,407.    Elapsed: 0:01:23.\n",
      "  Batch   600  of  1,407.    Elapsed: 0:01:29.\n",
      "  Batch   640  of  1,407.    Elapsed: 0:01:35.\n",
      "  Batch   680  of  1,407.    Elapsed: 0:01:41.\n",
      "  Batch   720  of  1,407.    Elapsed: 0:01:48.\n",
      "  Batch   760  of  1,407.    Elapsed: 0:01:54.\n",
      "  Batch   800  of  1,407.    Elapsed: 0:02:00.\n",
      "  Batch   840  of  1,407.    Elapsed: 0:02:05.\n",
      "  Batch   880  of  1,407.    Elapsed: 0:02:11.\n",
      "  Batch   920  of  1,407.    Elapsed: 0:02:17.\n",
      "  Batch   960  of  1,407.    Elapsed: 0:02:23.\n",
      "  Batch 1,000  of  1,407.    Elapsed: 0:02:29.\n",
      "  Batch 1,040  of  1,407.    Elapsed: 0:02:35.\n",
      "  Batch 1,080  of  1,407.    Elapsed: 0:02:41.\n",
      "  Batch 1,120  of  1,407.    Elapsed: 0:02:47.\n",
      "  Batch 1,160  of  1,407.    Elapsed: 0:02:53.\n",
      "  Batch 1,200  of  1,407.    Elapsed: 0:02:59.\n",
      "  Batch 1,240  of  1,407.    Elapsed: 0:03:05.\n",
      "  Batch 1,280  of  1,407.    Elapsed: 0:03:11.\n",
      "  Batch 1,320  of  1,407.    Elapsed: 0:03:17.\n",
      "  Batch 1,360  of  1,407.    Elapsed: 0:03:23.\n",
      "  Batch 1,400  of  1,407.    Elapsed: 0:03:29.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epcoh took: 0:03:30\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.88\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,407.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  1,407.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  1,407.    Elapsed: 0:00:18.\n",
      "  Batch   160  of  1,407.    Elapsed: 0:00:24.\n",
      "  Batch   200  of  1,407.    Elapsed: 0:00:30.\n",
      "  Batch   240  of  1,407.    Elapsed: 0:00:36.\n",
      "  Batch   280  of  1,407.    Elapsed: 0:00:42.\n",
      "  Batch   320  of  1,407.    Elapsed: 0:00:48.\n",
      "  Batch   360  of  1,407.    Elapsed: 0:00:54.\n",
      "  Batch   400  of  1,407.    Elapsed: 0:01:00.\n",
      "  Batch   440  of  1,407.    Elapsed: 0:01:06.\n",
      "  Batch   480  of  1,407.    Elapsed: 0:01:12.\n",
      "  Batch   520  of  1,407.    Elapsed: 0:01:18.\n",
      "  Batch   560  of  1,407.    Elapsed: 0:01:23.\n",
      "  Batch   600  of  1,407.    Elapsed: 0:01:29.\n",
      "  Batch   640  of  1,407.    Elapsed: 0:01:35.\n",
      "  Batch   680  of  1,407.    Elapsed: 0:01:41.\n",
      "  Batch   720  of  1,407.    Elapsed: 0:01:47.\n",
      "  Batch   760  of  1,407.    Elapsed: 0:01:53.\n",
      "  Batch   800  of  1,407.    Elapsed: 0:01:59.\n",
      "  Batch   840  of  1,407.    Elapsed: 0:02:05.\n",
      "  Batch   880  of  1,407.    Elapsed: 0:02:11.\n",
      "  Batch   920  of  1,407.    Elapsed: 0:02:17.\n",
      "  Batch   960  of  1,407.    Elapsed: 0:02:23.\n",
      "  Batch 1,000  of  1,407.    Elapsed: 0:02:29.\n",
      "  Batch 1,040  of  1,407.    Elapsed: 0:02:35.\n",
      "  Batch 1,080  of  1,407.    Elapsed: 0:02:41.\n",
      "  Batch 1,120  of  1,407.    Elapsed: 0:02:47.\n",
      "  Batch 1,160  of  1,407.    Elapsed: 0:02:53.\n",
      "  Batch 1,200  of  1,407.    Elapsed: 0:02:59.\n",
      "  Batch 1,240  of  1,407.    Elapsed: 0:03:05.\n",
      "  Batch 1,280  of  1,407.    Elapsed: 0:03:11.\n",
      "  Batch 1,320  of  1,407.    Elapsed: 0:03:17.\n",
      "  Batch 1,360  of  1,407.    Elapsed: 0:03:23.\n",
      "  Batch 1,400  of  1,407.    Elapsed: 0:03:29.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epcoh took: 0:03:30\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.88\n",
      "  Validation took: 0:00:07\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,407.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  1,407.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  1,407.    Elapsed: 0:00:18.\n",
      "  Batch   160  of  1,407.    Elapsed: 0:00:24.\n",
      "  Batch   200  of  1,407.    Elapsed: 0:00:30.\n",
      "  Batch   240  of  1,407.    Elapsed: 0:00:36.\n",
      "  Batch   280  of  1,407.    Elapsed: 0:00:42.\n",
      "  Batch   320  of  1,407.    Elapsed: 0:00:48.\n",
      "  Batch   360  of  1,407.    Elapsed: 0:00:54.\n",
      "  Batch   400  of  1,407.    Elapsed: 0:01:00.\n",
      "  Batch   440  of  1,407.    Elapsed: 0:01:06.\n",
      "  Batch   480  of  1,407.    Elapsed: 0:01:12.\n",
      "  Batch   520  of  1,407.    Elapsed: 0:01:18.\n",
      "  Batch   560  of  1,407.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,407.    Elapsed: 0:01:30.\n",
      "  Batch   640  of  1,407.    Elapsed: 0:01:36.\n",
      "  Batch   680  of  1,407.    Elapsed: 0:01:42.\n",
      "  Batch   720  of  1,407.    Elapsed: 0:01:48.\n",
      "  Batch   760  of  1,407.    Elapsed: 0:01:54.\n",
      "  Batch   800  of  1,407.    Elapsed: 0:02:00.\n",
      "  Batch   840  of  1,407.    Elapsed: 0:02:06.\n",
      "  Batch   880  of  1,407.    Elapsed: 0:02:12.\n",
      "  Batch   920  of  1,407.    Elapsed: 0:02:18.\n",
      "  Batch   960  of  1,407.    Elapsed: 0:02:24.\n",
      "  Batch 1,000  of  1,407.    Elapsed: 0:02:30.\n",
      "  Batch 1,040  of  1,407.    Elapsed: 0:02:36.\n",
      "  Batch 1,080  of  1,407.    Elapsed: 0:02:43.\n",
      "  Batch 1,120  of  1,407.    Elapsed: 0:02:49.\n",
      "  Batch 1,160  of  1,407.    Elapsed: 0:02:55.\n",
      "  Batch 1,200  of  1,407.    Elapsed: 0:03:01.\n",
      "  Batch 1,240  of  1,407.    Elapsed: 0:03:07.\n",
      "  Batch 1,280  of  1,407.    Elapsed: 0:03:13.\n",
      "  Batch 1,320  of  1,407.    Elapsed: 0:03:19.\n",
      "  Batch 1,360  of  1,407.    Elapsed: 0:03:26.\n",
      "  Batch 1,400  of  1,407.    Elapsed: 0:03:32.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epcoh took: 0:03:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.88\n",
      "  Validation took: 0:00:07\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "loss_tr_hist = []\n",
    "accuracy_tr_hist = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss_tr_hist.append(total_loss)\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        accuracy_tr_hist.append(eval_accuracy/1563)\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.8882\n",
      "  Test took: 0:01:07\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "    \n",
    "    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "    # values prior to applying an activation function like the softmax.\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # Calculate the accuracy for this batch of test sentences.\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    # Accumulate the total accuracy.\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    # accuracy_tr_hist.append(eval_accuracy)\n",
    "\n",
    "    # Track the number of batches\n",
    "    nb_eval_steps += 1\n",
    "print(\"  Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"  Test took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_tr_hist)\n",
    "plt.savefig('acc.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# l = [i/1407 for i in loss_tr_hist]\n",
    "plt.plot(loss_tr_hist)\n",
    "plt.savefig('loss.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(628, 5628)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accuracy_tr_hist),len(loss_tr_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1563"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_eval_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_torch",
   "language": "python",
   "name": "tf_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bda35a1e879a05a447a165f858b45568b1b1e2abd2947b4d01b6ed4acf0b7308"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
